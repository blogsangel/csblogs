{
  "hash": "8f3630f55696f461e5be35274ef53132",
  "result": {
    "markdown": "---\ntitle: Clustering\nformat:\n  html:\n    code-fold: true\n---\n\n# Clustering\nClustering in Unservised learning Solution. where we have some data and we want to group them accrding to postion and method we use for clustering.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n\nGenerate datasets. We choose the size big enough to see the scalability\nof the algorithms, but not too big to avoid too long running times\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn_samples = 1000\n\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n```\n:::\n\n\nSet up cluster parameters\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndefault_base = {\n    \"quantile\": 0.3,\"eps\": 0.3,\"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 2,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n```\n:::\n\n\nWe have make a noisy circle data set for show the two cluster method on that. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05\n)\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    )\n\n]\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ni_dataset, (dataset, algo_params)=0,datasets[0]\n# update parameters with dataset-specific values\nparams = default_base.copy()\nparams.update(algo_params)\n\nX, y = dataset\n\n# normalize dataset for easier parameter selection\nX = StandardScaler().fit_transform(X)\n\n# estimate bandwidth for mean shift\nbandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n# connectivity matrix for structured Ward\nconnectivity = kneighbors_graph(\n    X, n_neighbors=params[\"n_neighbors\"], include_self=False\n)\n# make connectivity symmetric\nconnectivity = 0.5 * (connectivity + connectivity.T)\n```\n:::\n\n\nCreate cluster objects\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nkmeans = cluster.KMeans(\n    n_clusters=params[\"n_clusters\"],\n    n_init=\"auto\",\n    random_state=params[\"random_state\"],\n)\ndbscan = cluster.DBSCAN(eps=params[\"eps\"])\n\n\nclustering_algorithms = (\n    (\"KMeans\", kmeans),\n    (\"DBSCAN\", dbscan),\n)\n```\n:::\n\n\nWe apply KMeans Clustering and Dbscan method and show how it works on our data\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 3))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\nplot_num=1\nfor name, algorithm in clustering_algorithms:\n    t0 = time.time()\n\n    # catch warnings related to kneighbors_graph\n    algorithm.fit(X)\n\n    t1 = time.time()\n    if hasattr(algorithm, \"labels_\"):\n        y_pred = algorithm.labels_.astype(int)\n    else:\n        y_pred = algorithm.predict(X)\n\n    plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n    if i_dataset == 0:\n        plt.title(name, size=18)\n\n    colors = np.array(list(islice(cycle([\"#377eb8\",\"#ff7f00\",\"#4daf4a\",\"#f781bf\",\"#a65628\",\"#984ea3\",\"#999999\",\"#e41a1c\",\"#dede00\",]),int(max(y_pred) + 1),)))\n    # add black color for outliers (if any)\n    colors = np.append(colors, [\"#000000\"])\n    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n    plt.xlim(-2.5, 2.5)\n    plt.ylim(-2.5, 2.5)\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.99,0.01,(\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n        transform=plt.gca().transAxes,\n        size=15,horizontalalignment=\"right\",\n    )\n    plot_num += 1\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=669 height=324}\n:::\n:::\n\n\nKemans The Kmeas works to select the centeroid and select the N nearest cluster to the centroid each time so it have divided the circle into two groups\n\nDBScan: the Db Scan work on Density based it increase the cluster area based on density increase\n\n# References \n\n[1] https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n\n[2] https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/\n\n[3] https://www.geeksforgeeks.org/k-means-clustering-introduction/\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}