[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Myblog",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnomaly Detection\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProbability Theory and random\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/AnomalyDetection/index.html",
    "href": "posts/AnomalyDetection/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection is the process of identifying data points, entities or events that fall outside the normal range.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n\nMake Some Dummy Data and add Anomalites on that. we have added two type of anomally onw contane large scale and other have different loction. all Data from Normal Sitribution\n\n\nCode\n# Generate synthetic data with anomalies\nnp.random.seed(42)\nnormal_data = np.random.normal(0, 1, (1000, 2))\nanomalies = np.random.normal(20, 1, (50, 2))\nanomalies2 = np.random.normal(0, 5, (50, 2))\n# Combine normal and anomaly data\ndata = np.vstack([normal_data, anomalies,anomalies2])\n\n\nFind the Anomality with IsolationForest.\n\n\nCode\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Contamination is the proportion of outliers\nmodel.fit(data)\n\n# Predict the labels (1 for inliers, -1 for outliers)\nlabels = model.predict(data)\n\n# Plot the data and highlight anomalies\nplt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.colorbar()\nplt.show()\n\n\n\n\n\nFind the Anomality with IsolationForest\n\n\nCode\n# Fit the Isolation Forest model\nmodel = LocalOutlierFactor()  # Contamination is the proportion of outliers\n\n# Predict the labels (1 for inliers, -1 for outliers)\nlabels = model.fit_predict(data)\n\n# Plot the data and highlight anomalies\nplt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.colorbar()\nplt.show()\n\n\n\n\n\nwe have implement two Anomaly Detection methods, One is IsolationForest which randomly selecting a split value between the maximum and minimum values of the selected feature to find the outlier/ Anomaly from that. and another is LocalOutlierFactor which measures the local deviation of the density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood.\nReference links:\n[1]https://www.techtarget.com/searchenterpriseai/definition/anomaly-detection\n[2]https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor\n[3]https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest"
  },
  {
    "objectID": "posts/AnomalyDetection/index.html#anomaly-detection",
    "href": "posts/AnomalyDetection/index.html#anomaly-detection",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly detection is the process of identifying data points, entities or events that fall outside the normal range.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n\nMake Some Dummy Data and add Anomalites on that. we have added two type of anomally onw contane large scale and other have different loction. all Data from Normal Sitribution\n\n\nCode\n# Generate synthetic data with anomalies\nnp.random.seed(42)\nnormal_data = np.random.normal(0, 1, (1000, 2))\nanomalies = np.random.normal(20, 1, (50, 2))\nanomalies2 = np.random.normal(0, 5, (50, 2))\n# Combine normal and anomaly data\ndata = np.vstack([normal_data, anomalies,anomalies2])\n\n\nFind the Anomality with IsolationForest.\n\n\nCode\n# Fit the Isolation Forest model\nmodel = IsolationForest(contamination=0.05)  # Contamination is the proportion of outliers\nmodel.fit(data)\n\n# Predict the labels (1 for inliers, -1 for outliers)\nlabels = model.predict(data)\n\n# Plot the data and highlight anomalies\nplt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.colorbar()\nplt.show()\n\n\n\n\n\nFind the Anomality with IsolationForest\n\n\nCode\n# Fit the Isolation Forest model\nmodel = LocalOutlierFactor()  # Contamination is the proportion of outliers\n\n# Predict the labels (1 for inliers, -1 for outliers)\nlabels = model.fit_predict(data)\n\n# Plot the data and highlight anomalies\nplt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.colorbar()\nplt.show()\n\n\n\n\n\nwe have implement two Anomaly Detection methods, One is IsolationForest which randomly selecting a split value between the maximum and minimum values of the selected feature to find the outlier/ Anomaly from that. and another is LocalOutlierFactor which measures the local deviation of the density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood.\nReference links:\n[1]https://www.techtarget.com/searchenterpriseai/definition/anomaly-detection\n[2]https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor\n[3]https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification\nIn Classification we have Multi class Data and we want to classify the Data points class.\n\n\nCode\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nimport warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\n\n\nDefine Dataset\n\n\nCode\niris = load_iris()\nX, y = iris.data, iris.target\n\n\n\n\nCode\ndata = iris.data\ntarget = iris.target\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\niris_df = pd.DataFrame(data, columns=feature_names)\niris_df['Target'] = target_names[target]\n\nplt.figure()\n# Plot each feature against the target variable\nsns.pairplot(iris_df, hue='Target', markers=[\"o\", \"s\", \"D\"])\nplt.suptitle(\"Pairplot of Iris Features by Target\", y=1.02)\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\nLoading the Iris dataset here and split the data into training and testing sets\n\n\nCode\n# Load the Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\nCode\ndef DrawPricisionRecallF1Scor(y_test, y_pred,target_names):\n    warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning, module='sklearn.metrics')\n    # Calculate the classification report\n    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n    # Extract relevant metrics for each class\n    classes = list(iris.target_names)\n    precision = [report[c]['precision'] for c in classes]\n    recall = [report[c]['recall'] for c in classes]\n    f1_score = [report[c]['f1-score'] for c in classes]\n    \n    # Plotting the metrics on a histogram\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bar_width = 0.2\n    index = np.arange(len(classes))\n    \n    bar1 = ax.bar(index, precision, bar_width, label='Precision')\n    bar2 = ax.bar(index + bar_width, recall, bar_width, label='Recall')\n    bar3 = ax.bar(index + 2 * bar_width, f1_score, bar_width, label='F1-Score')\n    \n    ax.set_xlabel('Classes')\n    ax.set_ylabel('Scores')\n    ax.set_title('Classification Report Metrics by Class')\n    ax.set_xticks(index + bar_width)\n    ax.set_xticklabels(classes)\n    ax.legend()\n    \n    plt.show()\n\n\nTrain and Plot the Precision Recall,F1 Score for the maxdepth 1\n\n\nCode\n# Train a classifier (Random Forest in this example)\nclf = DecisionTreeClassifier(max_depth=1)\n#clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\n# Make predictions on the test set\nprint(f\"Accuracy is {clf.score(X_test,y_test):.2}%\")\ny_pred = clf.predict(X_test)\nDrawPricisionRecallF1Scor(y_test, y_pred,iris.target_names)\n\n\nAccuracy is 0.63%\n\n\n\n\n\nTrain and Plot the Precision Recall,F1 Score for the maxdepth 2\n\n\nCode\n# Train a classifier (Random Forest in this example)\nclf = DecisionTreeClassifier(max_depth=2)\n#clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\n# Make predictions on the test set\nprint(f\"Accuracy is {clf.score(X_test,y_test):.2}%\")\ny_pred = clf.predict(X_test)\nDrawPricisionRecallF1Scor(y_test, y_pred,iris.target_names)\n\n\nAccuracy is 0.97%\n\n\n\n\n\nTrain and Plot the Precision Recall, F1 Score for the maxdepth 3\n\n\nCode\n# Train a classifier (Random Forest in this example)\nclf = DecisionTreeClassifier(max_depth=3)\n#clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\n# Make predictions on the test set\nprint(f\"Accuracy is {clf.score(X_test,y_test):.2}%\")\ny_pred = clf.predict(X_test)\nDrawPricisionRecallF1Scor(y_test, y_pred,iris.target_names)\n\n\nAccuracy is 1.0%\n\n\n\n\n\nIn this article we have train the decision tree classifier on iris Dataset with different depth levels. and as per results it show\n63% accuracy with maxdepth=1\n96% accuracy with maxdepth=2\n100% accuracy with maxdepth=3\n\n\nReference links:\n[1] https://seaborn.pydata.org/generated/seaborn.pairplot.html\n[2] https://scikit-learn.org/stable/modules/tree.html#classification"
  },
  {
    "objectID": "posts/ProbabilityTheory/index.html",
    "href": "posts/ProbabilityTheory/index.html",
    "title": "Probability Theory and random",
    "section": "",
    "text": "Probability theory\na branch of mathematics concerned with the analysis of random data distributions. It helps us to understand the probabilities and likelihood of an event based on understanding distribution from their past events.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\nCreate a array with normal and exponential distribution\n\n\nCode\n# Number of samples\nnum_samples = 1000\n\n# Generate random data from a normal distribution\nrandom_data = np.random.normal(0,10,num_samples)\n\n# Generate random data from an exponential distribution\nscale_parameter = 10  # Adjust this parameter as needed\nexponential_data = np.random.exponential(scale=scale_parameter, size=num_samples)\n\n\nDraw the Probability distribution for both\n\n\nCode\n# Plotting the histograms\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.hist(random_data, bins=30, density=True, alpha=0.7, color='blue', label='Random (Normal)')\nplt.title('Random (Normal) Distribution')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.hist(exponential_data, bins=30, density=True, alpha=0.7, color='green', label='Exponential')\nplt.title('Exponential Distribution')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe probability density function produces the likelihood of values of the continuous random variable.It helps us to understand what is probability to get value between a particular range with given distribution\n\n\nCode\nx=np.sort(random_data)\npdf = norm.pdf(x, loc=0, scale=10)\n\n# Plot PDF\nplt.plot(x, pdf, color='blue', label='PDF of Normal Distribution')\nplt.title('Probability Density Function (PDF) of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx=np.sort(exponential_data)\npdf = norm.pdf(x, loc=0, scale=10)\n\n# Plot PDF\nplt.plot(x, pdf, color='blue', label='PDF of Normal Distribution')\nplt.title('Probability Density Function (PDF) of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n\n\n\n\n\nCDF finds the cumulative probability for the given value. it finds what is the probability to get the value low from the given value.\n\n\nCode\nx = np.sort(random_data)\ncdf = norm.cdf(x, loc=0, scale=10)\n\n# Plot CDF\nplt.plot(x, cdf, color='blue', label='CDF of Normal Distribution')\nplt.title('Cumulative Distribution Function (CDF) of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nx = np.sort(exponential_data)\ncdf = norm.cdf(x, loc=0, scale=10)\n\n# Plot CDF\nplt.plot(x, cdf, color='blue', label='CDF of Normal Distribution')\nplt.title('Cumulative Distribution Function (CDF) of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\nplt.legend()\nplt.show()\n\n\n\n\n\nProbability theory includes the methods to understand the data distribution. We have examine the pdf and Cdf of two different distributions one is random other is exponentiation\n\n\nReference links:\n[1] https://www.britannica.com/science/probability-theory\n[2] https://igppweb.ucsd.edu/~agnew/Courses/Sio223a/sio223a.chap2.pdf\n[3] https://numpy.org/doc/1.16/reference/routines.random.html\n[4] https://www.scribbr.com/statistics/normal-distribution\n[5] https://byjus.com/maths/probability-density-function\n[6] https://byjus.com/maths/cumulative-distribution-function/"
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression",
    "section": "",
    "text": "Regresion\na Supervised problem where we have some continues target to predict. In this Blog we have cover the Linear and nonliner regression on Two Linear and non linear data\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\nMake Linear Data\n\n\nCode\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X\n\n\nMake code to Plot\n\n\nCode\ndef MakePlot(X,y,y_pred,title=\"Nonlinear Regression Example\"):\n    plt.figure()\n    plt.scatter(X, y)\n    plt.plot(X, y_pred, \"r-\")\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Population\")\n    plt.title(title)\n    plt.show()\n\n\nLiner Regression model on Linear Data\n\n\nCode\nreg = LinearRegression().fit(X, y)\nprint(\"Accuracy\",int(reg.score(X, y)*100),\"%\")\n# print(reg.coef_,reg.intercept_)\ny_pred=reg.predict(X)\nMakePlot(X,y,y_pred,\"Linear Regression Model  On Linear Data\")\n\n\nAccuracy 100 %\n\n\n\n\n\nLiner Regression model on NonLinear Data\n\n\nCode\nX = np.linspace(0, 3, 100)\n# y = 2 * np.exp(1.5 * X)\ny = np.array([i*5+3 if i&gt;3 else i*50  for i in 2 * X])\nX,y=X[:,np.newaxis],y[:,np.newaxis]\n\n\n\n\nCode\nreg = LinearRegression().fit(X, y)\nprint(\"Accuracy\",int(reg.score(X, y)*100),\"%\")\nprint(reg.coef_,reg.intercept_)\ny_pred=reg.predict(X)\nMakePlot(X,y,y_pred,\"Linear Regression Model On NonLinear Data\")\n\n\nAccuracy 5 %\n[[-10.34653465]] [65.42889289]\n\n\n\n\n\nNonLiner Regression model on NonLinear Data\n\n\nCode\nfrom sklearn.tree import DecisionTreeRegressor\n\nreg = DecisionTreeRegressor().fit(X, y)\nprint(\"Accuracy\",int(reg.score(X, y)*100),\"%\")\ny_pred=reg.predict(X)\nMakePlot(X,y,y_pred,\"NonLinear Regression Model On NonLinear Data\")\n\n\nAccuracy 100 %\n\n\n\n\n\nLinear Regression Models work well if our Dependent variable is linearly correlated to Independent variables. However, when we try that on Nonlinear data where dependent is not linearly correlated the score is very low, but when we apply some nonlinear model (Decision Tree) on it we got a good regression Score.\n\n\nReference links:\n[1] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n[2] https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering\nClustering in Unservised learning Solution. where we have some data and we want to group them accrding to postion and method we use for clustering.\n\n\nCode\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\n\nGenerate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times\n\n\nCode\nn_samples = 1000\n\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n\nSet up cluster parameters\n\n\nCode\ndefault_base = {\n    \"quantile\": 0.3,\"eps\": 0.3,\"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 2,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\n\nWe have make a noisy circle data set for show the two cluster method on that.\n\n\nCode\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05\n)\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    )\n\n]\n\n\n\n\nCode\ni_dataset, (dataset, algo_params)=0,datasets[0]\n# update parameters with dataset-specific values\nparams = default_base.copy()\nparams.update(algo_params)\n\nX, y = dataset\n\n# normalize dataset for easier parameter selection\nX = StandardScaler().fit_transform(X)\n\n# estimate bandwidth for mean shift\nbandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n# connectivity matrix for structured Ward\nconnectivity = kneighbors_graph(\n    X, n_neighbors=params[\"n_neighbors\"], include_self=False\n)\n# make connectivity symmetric\nconnectivity = 0.5 * (connectivity + connectivity.T)\n\n\nCreate cluster objects\n\n\nCode\nkmeans = cluster.KMeans(\n    n_clusters=params[\"n_clusters\"],\n    n_init=\"auto\",\n    random_state=params[\"random_state\"],\n)\ndbscan = cluster.DBSCAN(eps=params[\"eps\"])\n\n\nclustering_algorithms = (\n    (\"KMeans\", kmeans),\n    (\"DBSCAN\", dbscan),\n)\n\n\nWe apply KMeans Clustering and Dbscan method and show how it works on our data\n\n\nCode\nplt.figure(figsize=(7, 3))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\nplot_num=1\nfor name, algorithm in clustering_algorithms:\n    t0 = time.time()\n\n    # catch warnings related to kneighbors_graph\n    algorithm.fit(X)\n\n    t1 = time.time()\n    if hasattr(algorithm, \"labels_\"):\n        y_pred = algorithm.labels_.astype(int)\n    else:\n        y_pred = algorithm.predict(X)\n\n    plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n    if i_dataset == 0:\n        plt.title(name, size=18)\n\n    colors = np.array(list(islice(cycle([\"#377eb8\",\"#ff7f00\",\"#4daf4a\",\"#f781bf\",\"#a65628\",\"#984ea3\",\"#999999\",\"#e41a1c\",\"#dede00\",]),int(max(y_pred) + 1),)))\n    # add black color for outliers (if any)\n    colors = np.append(colors, [\"#000000\"])\n    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n    plt.xlim(-2.5, 2.5)\n    plt.ylim(-2.5, 2.5)\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.99,0.01,(\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n        transform=plt.gca().transAxes,\n        size=15,horizontalalignment=\"right\",\n    )\n    plot_num += 1\n\nplt.show()\n\n\n\n\n\nKemans The Kmeas works to select the centeroid and select the N nearest cluster to the centroid each time so it have divided the circle into two groups\nDBScan: the Db Scan work on Density based it increase the cluster area based on density increase\n\n\nReferences\n[1] https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n[2] https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/\n[3] https://www.geeksforgeeks.org/k-means-clustering-introduction/"
  }
]